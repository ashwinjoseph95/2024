<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>IROS 2024: Workshop on Nonverbal Cues Analyses for Human-Robot Cooperative Intelligence in the Wild</title>
    <link rel="stylesheet" type="text/css" href="/2024/css/bootstrap.min.css">
    <link rel="stylesheet" type="text/css" href="/2024/css/main.css">
</head>
<body>
<nav class="navbar navbar-expand-lg navbar-light bg-light">
    <div class="container justify-content-center">
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent"
                aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse flex-grow-0" id="navbarSupportedContent">
            <ul class="navbar-nav">
                <li class="nav-item">
                    <a class="nav-link" href="#abstract">Abstract</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#topics">Topics Covered</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#review">Review Timeline</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#schedule">Schedule</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#organizers">Organizers</a>
                </li>
            </ul>
        </div>
    </div>
</nav>
<div class="container" style="max-width: 960px;">
    <div class="row mb-4 border-bottom">
        <div class="col-xs-12">
            <h1>ICRA 2024: Workshop on Nonverbal Cues Analyses for Human-Robot Cooperative Intelligence in the Wild</h1>
        </div>
    </div>

    <div class="row">
        <div class="col-xs-12">
            <a class="anchor" id="abstract"></a>
            <h2>Abstract</h2>
        </div>
    </div>
    <div class="row mb-4">
        <div class="col-xs-12">
            <p class="mb-0">Humans can perceive social cues and the interaction context of another human to infer the
                internal states
                including cognitive and emotional states, empathy, and intention. This unique ability to infer internal
                states leads to effective social interaction between humans desirable in many intelligent systems such
                as collaborative and social robots, and humanmachine interaction systems. However, it is challenging for
                machines to perceive human states under noisy real-world settings, which are usually measured by
                noninvasive sensors. Recent works investigating the potential solutions for the estimation of human
                states under controlled conditions using facial features with the off-the-shelf camera by leveraging
                deep learning methods. This workshop aims to bring interdisciplinary researchers across computer vision,
                artificial intelligence, robotics, and human-computer interaction together to share current research
                achievements and discuss future research directions for human behavior and state understanding in the
                wild, and their potential applications. Specifically, we are interested in cognition-aware computing by
                integrating environment contexts and multi-modal nonverbal social cues like gaze interaction, body
                language and para language. More importantly, we extend multi-modal human behavior research to infer the
                internal states of humans. This is a challenging problem yet important to realize effective interaction
                between humans and intelligent systems in the wild.
            </p>
            <p class="mb-0"><b>Keywords:</b> “Human: Face, gaze, body, pose, gesture, movement, attention, cognitive
                state, emotion
                state, intention, empathy, Environment: Object”
            </p>
            <p class="mb-0"><b>Secondary subject:</b> “Human-Robot cooperative intelligence”, “Nonverbal cues
                recognition from
                audiovisual”, “Human internal state inference from multi-modality”, “Vision applications and systems”,
                “Human-Object interaction and scene understanding”</p>
        </div>
    </div>

    <div class="row">
        <div class="col-xs-12">
            <a class="anchor" id="topics"></a>
            <h2>Topics Covered</h2>
        </div>
    </div>
    <div class="row mb-4">
        <div class="col-xs-12">
            <p class="mb-0">It is desirable for intelligent systems like robots, virtual agents, human-machine
                interfaces to collaborate and interact seamlessly with humans in the era of Industry 5.0, where
                intelligent systems must work alongside humans to perform a variety of tasks anywhere at home,
                factories, offices, transit, etc. The underlying technologies to achieve efficient and intelligent
                collaboration between humans and ubiquitous intelligent systems can be realized by cooperative
                intelligence [32], spanning interdisciplinary studies between robotics, artificial intelligence,
                human-robot and -computer interaction, computer vision, cognitive science, etc. </p>
            <p class="mb-0">&nbsp;&nbsp;&nbsp;&nbsp;One of the main considerations to achieve cooperative intelligence
                between humans and
                intelligent systems is to enable everyone and everything to know each other well, like how humans can
                trust or infer the implicit internal states like intention, emotion, and cognitive states of each other.
                The importance of empathy to facilitate human-robot interaction has been highlighted in previous studies
                [22, 29]. However, it is difficult for intelligent systems to estimate the internal states of humans
                because they are dependent on the complex social dynamics and environment contexts. This requires
                intelligent systems to be capable of sensing the multi-modal inputs, perceiving the salient inputs,
                reasoning the underlying abstract knowle</p>
            <p class="mb-0">&nbsp;&nbsp;&nbsp;&nbsp;There are many studies on estimating internal states of humans
                through measurements of
                wearables and non-invasive sensors [11, 44], but it would be difficult to implement these solutions in
                the wild because of the additional sensors to be worn by humans. One promising solution is to use
                audiovisual data like nonverbal behavior cues consisting of gaze interaction, facial expression, body
                language and paralanguage to infer the internal states of humans. Researchers in cognitive and social
                psychology have long advocated that these nonverbal behaviors are subconsciously generated by humans and
                reflect the internal states of humans under different contexts. Some salient examples are the studies on
                emotion recognition using facial [1] and body language [39, 42] in controlled environment. It remains an
                open question for intelligent systems to sense and recognize nonverbal cues and reason the implicit rich
                underlying internal states of humans in the wild and noisy environments.</p>
            <p>&nbsp;&nbsp;&nbsp;&nbsp;This workshop is dedicated to discussing computational methods for sensing and
                recognition of
                nonverbal cues and internal states in the wild to realize cooperative intelligence between humans and
                intelligent systems. We gather researchers from different expertise, yet having the common goal,
                motivation, and resolve to explore and tackle this delicate issue considering the practicality of
                industrial applications. We are calling for papers to discuss novel methods not limited to topics listed
                below.</p>
            <ul>
                <li>Robust sensing of facial and body key points.</li>
                <li>Recognition of nonverbal cues, e.g., gaze attention, body language, paralanguage.</li>
                <li>Human cognitive/emotional states inference/modeling.</li>
                <li>Multi-modal sensing fusion for scene perception.</li>
                <li>Social/Group interaction dynamics modeling, e.g., harmony level, engagements.</li>
                <li>Nonverbal behavior generation, e.g., gaze saliency, gesture.</li>
                <li>Synchronization of nonverbal and verbal behavior.</li>
                <li>Intention prediction.</li>
                <li>Personalization of intelligent systems from nonverbal cues and trust evaluation.</li>
                <li>Empathy/Cognitive interaction between humans and intelligent systems/robots.</li>
                <li> Learning algorithms for cooperative intelligence, e.g., representation learning, imitation
                    learning.
                </li>
                <li>Generative and adversarial training algorithms for human-robot interaction.</li>
                <li>Applications of cooperative intelligence in the wild.</li>
            </ul>
        </div>
    </div>

    <div class="row">
        <div class="col-xs-12">
            <a class="anchor" id="review"></a>
            <h2>Review Timeline</h2>
        </div>
    </div>
    <div class="row mb-4">
        <div class="col-xs-12">
            <ul>
                <li>Notification of workshop acceptance from IROS2024: <b>May 31, 2024</b></li>
                <li>Submission of final title and abstract to IROS2024: <b>June 15, 2024</b></li>
                <li>Workshop web page open: <b>June 15, 2024</b></li>
                <li>IROS2024 final decisions to authors: <b>June 30, 2024</b></li>
                <li>Workshop paper submission deadline: <b>July 15, 2024</b></li>
                <li>Workshop paper reviews deadline: <b>August 6, 2024</b></li>
                <li>Notification to authors: <b>August 21, 2024</b></li>
                <li>Camera-ready deadline: <b>August 31, 2024</b></li>
                <li>Submission of the URL of workshop’s website containing an updated schedule to IROS2024:
                    <b>September 01, 2024</b></li>
                <li>Workshop preferred date: <b>October 14, 2024</b></li>
            </ul>
        </div>
    </div>

    <div class="row">
        <div class="col-xs-12">
            <a class="anchor" id="schedule"></a>
            <h2>Schedule</h2>
        </div>
    </div>
    <div class="row mb-4">
        <div class="col-xs-12">
            <p>We plan a half-day event for 4 hours, including talks by two invited speakers, and one demo/interactive
                session. For participants who could not attend in-person, we will live stream the workshop events
                (excluding poster
                session) on our workshop page. Online participants can Q&A on the comment section.</p>
            <ul>
                <li>Welcome and opening remarks (5 mins)</li>
                <li>Invited talk I (40 mins, including 5 mins Q&A)</li>
                <li>Invited talk II (40 mins, including 5 mins Q&A)</li>
                <li>Break and demo/interactive session (20 mins)</li>
                <li>Accepted workshop papers (100 mins) (About 10 papers, 10 mins each)</li>
                <li>Poster session (30 mins)</li>
                <li>Closing remarks (5 mins)</li>
            </ul>
        </div>
    </div>

    <div class="row">
        <div class="col-xs-12">
            <a class="anchor" id="organizers"></a>
            <h2>Organizers</h2>
        </div>
    </div>
    <div class="row mb-4">
        <div class="col-xl-3 mb-4">
            <div class="organizer-name">
                <a href="https://www.jp.honda-ri.com/en/members/chew-jouh-yeong/">Jouh Yeong Chew</a>
                <h6>Honda Research Institute Japan</h6>
                <a href="mailto:jouhyeong.chew@jp.honda-ri.com">jouhyeong.chew@jp.honda-ri.com</a>
                <h6>(point of contact)</h6>
            </div>
        </div>
        <div class="col-xl-3 mb-4">
            <div class="organizer-name">
                <a href="homepage">Andreas Bulling</a>
                <h6>University of Stuttgart</h6>
                <a href="mailto:andreas.bulling@vis.uni-stuttgart.de">andreas.bulling@vis.uni-stuttgart.de</a>
            </div>
        </div>
        <div class="col-xl-3 mb-4">
            <div class="organizer-name">
                <a href="homepage">Daisuke Kurabayashi</a>
                <h6>Tokyo Institute of Technology</h6>
                <a href="mailto:kurabayashi.d.aa@m.titech.ac.jp">kurabayashi.d.aa@m.titech.ac.jp</a>
            </div>
        </div>
        <div class="col-xl-3 mb-4">
            <div class="organizer-name">
                <a href="homepage">Eiichi Yoshida</a>
                <h6>Tokyo University of Science</h6>
                <a href="mailto:eiichi.yoshida@rs.tus.ac.jp">eiichi.yoshida@rs.tus.ac.jp</a>
            </div>
        </div>
        <div class="col-xl-3 mb-4">
            <div class="organizer-name">
                <a href="homepage">Iolanda Leite</a>
                <h6>KTH Royal Institute of Technology</h6>
                <a href="mailto:iolanda@kth.se">iolanda@kth.se</a>
            </div>
        </div>
        <div class="col-xl-3 mb-4">
            <div class="organizer-name">
                <a href="homepage">Siyu Tang</a>
                <h6>ETH Z¨urich</h6>
                <a href="mailto:siyu.tang@inf.ethz.ch">siyu.tang@inf.ethz.ch</a>
            </div>
        </div>
        <div class="col-xl-3 mb-4">
            <div class="organizer-name">
                <a href="homepage">Xucong Zhang</a>
                <h6>TU Delft</h6>
                <a href="mailto:xucong.zhang@tudelft.nl">xucong.zhang@tudelft.nl</a>
            </div>
        </div>
    </div>
</div>
<script src="/2024/js/bootstrap.min.js"></script>

</body>
</html>