<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>IROS 2024: Workshop on Nonverbal Cues for Human-Robot Cooperative Intelligence</title>
    
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css"> 
    <!--  <link rel="stylesheet" type="text/css" href="css/bootstrap.min.css"> --> 
    <link rel="stylesheet" type="text/css" href="css/main.css"> 
</head>
<body>
<nav class="navbar navbar-expand-lg navbar-light bg-light fixed-top shadow">
    <!-- <div class="navbar-title"><a href="#">IROS 2024</a></div> -->
    <a class="navbar-title" href="#">
        <img src="images/logo_iros.png" width="40" height="40" class="shadow-sm">
    </a>
    <div class="container">
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent"
                aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse flex-grow-0" id="navbarSupportedContent">
            <ul class="navbar-nav">
                <li class="nav-item">
                    <a class="nav-link" href="#abstract">Abstract</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#topics">Topics Covered</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#submissions">Submissions</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#review">Review Timeline</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#schedule">Schedule</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#organizers">Organizers</a>
                </li>
            </ul>
        </div>
    </div>
    <a class="navbar-right-log", href="https://www.jp.honda-ri.com/en/", target="_blank">
        <img src="images/logo_hri.png" width="50" height="20">
    </a>
</nav>
<div class="container" style="max-width: 960px; margin-top: 100px;">
    <div class="jumbotron">
        <h1 class="anchor"><span class="highlight1">Workshop</span> on Nonverbal Cues for Human-Robot Cooperative Intelligence</h1>
        <img src="images/banner.png">
        <p></p>
    </div>

    <div class="row">
        <div class="col-xs-12">
            <a class="anchor" id="abstract"></a>
            <h2 class="h1-bullet">Abstract</h2>
        </div>
    </div>
    <div class="row mb-4">
        <div class="col-xs-12">
            <p class="mb-0">
                Humans can perceive social cues and the interaction context of another human to infer
                the internal states including cognitive and emotional states, empathy, and intention. This
                unique ability to infer internal states leads to effective social interaction between humans
                desirable in many intelligent systems such as collaborative and social robots, and humanmachine
                interaction systems. However, it is challenging for machines to perceive human
                states under noisy real-world settings, which are usually measured by noninvasive sensors.
                Recent works investigating the potential solutions for the estimation of human states under controlled
                conditions using facial features with the off-the-shelf camera by leveraging
                deep learning methods. This workshop aims to bring interdisciplinary researchers across
                computer vision, artificial intelligence, robotics, and human-computer interaction together
                to share current research achievements and discuss future research directions for human
                behavior and state understanding, and their potential application, especially in the wild
                environment. Specifically, we are interested in cognition-aware computing by integrating
                environment contexts and multi-modal nonverbal social cues not limited to gaze interaction, body
                language and para language. More importantly, we extend multi-modal human
                behavior research to infer the internal states of humans. This is a challenging problem yet
                important to realize effective interaction between humans and intelligent systems.
            </p>
            <p class="mb-0"><b>Keywords:</b> "Human: Face, gaze, body, pose, gesture, movement, attention,
                cognitivestate, emotion state, intention, empathy, Environment: Object"
            </p>
            <p class="mb-0"><b>Secondary subject:</b> "Human-Robot cooperative intelligence", "Nonverbal cues
                recognition from audiovisual", "Human internal state inference from multi-modality", "Vision
                applications and systems", "Human-Object interaction and scene understanding"</p>
        </div>
    </div>

    <div class="row">
        <div class="col-xs-12">
            <a class="anchor" id="topics"></a>
            <h2 class="h1-bullet">Topics Covered</h2>
        </div>
    </div>
    <div class="row mb-4">
        <div class="col-xs-12">
            <p class="mb-0">It is desirable for intelligent systems like robots, virtual agents, human-machine
                interfaces to collaborate and interact seamlessly with humans in the era of Industry 5.0, where
                intelligent systems must work alongside humans to perform a variety of tasks anywhere at home,
                factories, offices, transit, etc. The underlying technologies to achieve efficient and intelligent
                collaboration between humans and ubiquitous intelligent systems can be realized by cooperative
                intelligence, spanning interdisciplinary studies between robotics, AI,
                human-robot and -computer interaction, computer vision, cognitive science, etc. </p>
            <p class="mb-0">&nbsp;&nbsp;&nbsp;&nbsp;One of the main considerations to achieve cooperative intelligence
                between humans and
                intelligent systems is to enable everyone and everything to know each other well, like how humans can
                trust or infer the implicit internal states like intention, emotion, and cognitive states of each other.
                The importance of empathy to facilitate human-robot interaction has been highlighted in previous studies
                . However, it is difficult for intelligent systems to estimate the internal states of humans
                because they are dependent on the complex social dynamics and environment contexts. This requires
                intelligent systems to be capable of sensing the multi-modal inputs,
                reasoning the underlying abstract knowledge, and generating the corresponding responses to collaborate
                and interact with humans.</p>
            <p class="mb-0"> &nbsp;&nbsp;&nbsp;&nbsp;There are many studies on estimating internal states of humans
                through measurements of wearables and
                non-invasive sensors, but it would be difficult to implement these solutions in the wild because of the
                additional sensors to be worn by humans. One promising solution is to use audiovisual data like
                nonverbal behavior cues consisting of gaze interaction, facial expression, body language and
                paralanguage to infer the internal states of humans. Researchers in cognitive and social psychology have
                long advocated that these nonverbal behaviors are subconsciously generated by humans and reflect the
                internal states of humans under different contexts. Some salient examples are the studies on emotion
                recognition using facial and body language in controlled environment. It remains an open question for
                intelligent systems to sense and recognize nonverbal cues and reason the rich underlying internal states
                of humans in the wild and noisy environments. </p>
            <p class="mb-0">&nbsp;&nbsp;&nbsp;&nbsp;This workshop is dedicated to discussing computational methods for
                sensing and
                recognition of
                nonverbal cues and internal states in the wild to realize cooperative intelligence between humans and
                intelligent systems. We gather researchers from different expertise, yet having the common goal,
                motivation, and resolve to explore and tackle this delicate issue considering the practicality of
                industrial applications. We are calling for papers to discuss novel methods to realize human-robot
                cooperative intelligence by sensing and understanding humans’ behavior, internal states, and to generate
                empathetic interactions. </p>
            <ul class="topic-list">
                <li>Human internal state inference, e.g., cognitive, emotional, intention models.</li>
                <li>Recognition of nonverbal cues, e.g., gaze and attention, body language, para-language.</li>
                <li>Multi-modal sensing fusion for scene perception.</li>
                <li>Nonverbal behavior generation for robots/agents, e.g., gaze salience, gesture.</li>
                <li>Synchronization of nonverbal and verbal behavior</li>
                <li>Learning algorithms for cooperative intelligence, e.g., imitation learning.</li>
                <li>Generative and adversarial algorithms to enhance human-robot interaction, e.g., LLMs, diffusion
                    models, VLMs.
                </li>
                <li>Empathetic interaction between humans and intelligent systems.</li>
                <li>Robust sensing of facial and body key points.</li>
                <li>Social interaction dynamics modeling, e.g., harmony level, engagements.</li>
                <li>Personalization of intelligent systems from nonverbal cues and trust evaluation.</li>
                <li>Applications of cooperative intelligence in the wild.</li>
            </ul>            
        </div>
       
    </div>

    <div class="row">
        <div class="col-xs-12">
            <a class="anchor" id="submissions"></a>
            <h2 class="h1-bullet">Submissions</h2>
        </div>
    </div>
    <div class="col-xs-12 content">
        <p>We invite authors to submit unpublished papers
        (<a target="_blank" href="https://www.ieee.org/conferences/publishing/templates.html">IEEE format</a>) 
        to our workshop, to be presented at a workshop session upon acceptance. 
        Submissions will undergo a double-blind review process and must be submitted via EasyChair. 
        Three-way presentation approach is designed to foster active participation </p>
        <ul class="topic-list">
            <li>Spotlight talks (7 mins talk + 3 mins Q&A) </li>
            <li>In-person posters for in-depth discussions</li>
            <li>Short pre-recorded videos (about 3 minutes) on the workshop page</li>
        </ul>        
    </div>
    <div class="col-xs-12">
        <div class="submition-section">
            <a target="_blank" href="https://easychair.org/my/conference?conf=iros2024workshoponno">Submit papers on EasyChair</a>
        </div>
    </div>

    <div class="row">
        <div class="col-xs-12">
            <a class="anchor" id="review"></a>
            <h2 class="h1-bullet">Review Timeline</h2>
        </div>
    </div>
    <div class="row mb-4">
        <div class="col-xs-12">
            <ul class="timeline">
                <li>
                    <span class="date-label">June 18, 2024 PST</span>
                    <p>Notification of workshop acceptance</p>
                </li>
                <li>
                    <span class="date-label">June 15, 2024 PST</span>
                    <p>Submission of final title and abstract to IROS 2024</p>
                </li>
                <li>
                    <span class="date-label">July 1, 2024 PST</span>
                    <p>Workshop webpage open with submission </p>
                </li>
                <li>
                    <span class="date-label">July 31, 2024 PST</span>
                    <p>Workshop paper submission deadline</p>
                </li>
                <li>
                    <span class="date-label">August 21, 2024 PST</span>
                    <p>Workshop paper reviews deadline </p>
                </li>
                <li>
                    <span class="date-label">August 31, 2024 PST</span>
                    <p>Notification to authors </p>
                </li>
                <li>
                    <span class="date-label">September 14, 2024 PST</span>
                    <p>Camera-ready deadline </p>
                </li>
                <li>
                    <span class="date-label">September 28, 2024 PST</span>
                    <p>Pre-recorded video deadline</p>
                </li>                
                <li>
                    <span class="date-label">September 1, 2024 PST</span>
                    <p>Submission of the URL of workshop’s website
                    </p>
                </li>                
                <li>
                    <span class="date-label">October 14, 2024 PST</span>
                    <p>Workshop preferred date</p>
    </div>

    <div class="row">
        <div class="col-xs-12">
            <a class="anchor" id="schedule"></a>
            <h2 class="h1-bullet">Schedule</h2>
        </div>
    </div>
    <div class="row mb-4">
        <div class="col-xs-12">
            <p>We plan a half-day event for 4 hours, including talks by two invited speakers, and one interactive
                session. For participants who could not attend in person, we will disseminate the papers and
                pre-recorded videos on our workshop page, which also consists of a comment section for Q&A.</p>
            <ul class="schelude">
                <li>
                    <span class="time-bg">08:30</span>
                    <span class="ribbon">08:35</span> 
                    <span class="schelude-item-content">Welcome and opening remarks (5 mins)</span>
                </li>
                <li>
                    <span class="time-bg">08:35</span>
                    <span class="ribbon">09:15</span> 
                    <span class="schelude-item-content">Invited talk I (40 mins, including 5 mins Q&A)</span>
                </li>
                <li>
                    <span class="time-bg">09:15</span>
                    <span class="ribbon">09:45</span> 
                    <span class="schelude-item-content">Accepted workshop papers (3 papers, 10 mins each)</span>
                </li>
                <li>
                    <span class="time-bg">09:45</span>
                    <span class="ribbon">10:05</span> 
                    <span class="schelude-item-content">Invited IROS2024 papers (2 papers, 10 mins each)</span>
                </li>
                <li>
                    <span class="time-bg">10:05</span>
                    <span class="ribbon">10:40</span> 
                    <span class="schelude-item-content">Coffee break and poster session (35 mins)</span>
                </li>
                <li>
                    <span class="time-bg">10:40</span>
                    <span class="ribbon">11:20</span> 
                    <span class="schelude-item-content">Invited talk II (40 mins, including 5 mins Q&A)</span>
                </li>
                <li>
                    <span class="time-bg">11:20</span>
                    <span class="ribbon">11:50</span> 
                    <span class="schelude-item-content">Accepted workshop papers (3 papers, 10 mins each)</span>
                </li>
                <li>
                    <span class="time-bg">11:50</span>
                    <span class="ribbon">12:25</span> 
                    <span class="schelude-item-content">Interactive session (35 mins)</span>
                </li>
                <li>
                    <span class="time-bg">12:25</span>
                    <span class="ribbon">12:30</span> 
                    <span class="schelude-item-content">Closing remarks (5 mins)</span>
                </li>
            </ul>
        </div>
    </div>

    <div class="row">
        <div class="col-xs-12">
            <a class="anchor" id="invited_speakers"></a>
            <h2 class="h1-bullet">Invited Speakers</h2>
        </div>
    </div>
    <div class="row mb-4 speaker-row">
        <div class="col-xs-12">
            <p>
                We intend to have speakers from different ethnic backgrounds, countries, and career stages.
                Specifically, we confirmed the attendance of one speaker from the industry, and the other speaker from
                the academia is pending confirmation.
            </p>
        </div>
        <div class="row mb-4">
            <div class="col-xl-3 mb-4">
                <img src="images/speaker_1.jpg" class="img-speaker shadow">
            </div>
            <div class="col-xl-9 mb-4">
                <div><span>Satoshi Shigemi, </span> Honda Research Institute Japan, Japan.</div>
                <div>Link to website: <a href="http://www.jp.honda-ri.com/en/about/">http://www.jp.honda-ri.com/en/about/</a>
                </div>
                <div class="speaker-content">Satoshi Shigemi is the President of Honda Research Institute Japan. Since 1987, he has
                    been conducting research on robots and control systems at Honda R&D Co. In 2000, he was
                    the Senior Chief Engineer and project lead for the research and development of ASIMO,
                    the humanoid robot. He then developed a high-altitude survey robot for the Fukushima
                    Daiichi Nuclear Power Plant. He has published many papers about human-robot interaction.
                    Satoshi has tentatively confirmed to be the speaker at our workshop.
                    The preliminary title is “Co-Existence with Intelligent Machines”.
                </div>
            </div>
        </div>
        <div class="row mb-4">
            <div class="col-xl-3 mb-4">
                <!-- <img src="images/speaker_2.jpg" class="img-speaker shadow"> -->
            </div>
            <div class="col-xl-9 mb-4">
                <div>
                    <i>To be confirmed </i>
                    <!-- <span>Jean-Marc Odobez, </span> IDIAP Research Institute and EPFL, Switzerland. -->
                </div>
                <!-- <div>Link to website: <a href=": https://www.idiap.ch/~odobez/">: https://www.idiap.ch/~odobez/</a>
                </div> -->
                <div class="speaker-content">
                    <!-- Jean-Marc Odebez is the leader of the Perception and Activity Understanding group at the
                    Idiap Research Institute, EPFL. His research interests are in human activities analysis from
                    multi-modal data. This entails the investigation of fundamental tasks like the detection
                    and tracking of people, the estimation of their pose or the detection of non-verbal behaviors, and
                    the temporal interpretation of this information in forms of gestures, activities,
                    behavior or social relationships. These tasks are addressed through the design of principled
                    algorithms extending models from computer vision, multimodal signal processing,
                    and machine learning, in particular probabilistic graphical models and deep learning
                    techniques. Jean-Marc has not confirmed his attendance as a speaker. -->
                </div>
            </div>
        </div>
    </div>

    <div class="row">
        <div class="col-xs-12">
            <a class="anchor" id="organizers"></a>
            <h2 class="h1-bullet">Organizers</h2>
        </div>
    </div>
    <div class="row mb-4">
        <div class="col-xl-3 mb-4">
            <div class="organizer-card shadow-sm">
                <h5 class="card-title">Jouh Yeong Chew</h5>
                <p class="card-text">Honda Research Institute Japan</p>
                <a href="mailto:jouhyeong.chew@jp.honda-ri.com">jouhyeong.chew@jp.honda-ri.com</a>
            </div>
        </div>
        <div class="col-xl-3 mb-4">
            <div class="organizer-card shadow-sm">
                <h5 class="card-title">Andreas Bulling</h5>
                <p class="card-text">University of Stuttgart</p>
                <a href="mailto:andreas.bulling@vis.uni-stuttgart.de">andreas.bulling@vis.uni-stuttgart.de</a>
            </div>
        </div>
        <div class="col-xl-3 mb-4">
            <div class="organizer-card shadow-sm">
                <h5 class="card-title">Daisuke Kurabayashi</h5>
                <p class="card-text">Tokyo Institute of Technology</p>
                <a href="mailto:kurabayashi.d.aa@m.titech.ac.jp">kurabayashi.d.aa@m.titech.ac.jp</a>
            </div>
        </div>
        <div class="col-xl-3 mb-4">
            <div class="organizer-card shadow-sm">
                <h5 class="card-title">Eiichi Yoshida</h5>
                <p class="card-text">Tokyo University of Science</p>
                <a href="mailto:eiichi.yoshida@rs.tus.ac.jp">eiichi.yoshida@rs.tus.ac.jp</a>
            </div>
        </div>
        <div class="col-xl-3 mb-4">
            <div class="organizer-card shadow-sm">
                <h5 class="card-title">Iolanda Leite</h5>
                <p class="card-text">KTH Royal Institute of Technology</p>
                <a href="mailto:iolanda@kth.se">iolanda@kth.se</a>
            </div>
        </div>
        <div class="col-xl-3 mb-4">
            <div class="organizer-card shadow-sm">
                <h5 class="card-title">Siyu Tang</h5>
                <p class="card-text">ETH Z¨urich</p>
                <a href="mailto:siyu.tang@inf.ethz.ch">siyu.tang@inf.ethz.ch</a>
            </div>
        </div>
        <div class="col-xl-3 mb-4">
            <div class="organizer-card shadow-sm">
                <h5 class="card-title">Xucong Zhang</h5>
                <p class="card-text">TU Delft</p>
                <a href="mailto:xucong.zhang@tudelft.nl">xucong.zhang@tudelft.nl</a>
            </div>
        </div>
    </div>
</div>

<footer class="d-flex flex-wrap justify-content-between align-items-center py-3 my-4 border-top">
    <div class="col-md-8 d-flex align-items-center">
        <a target="_blank" href="https://www.jp.honda-ri.com/en/" class="mb-3 me-2 mb-md-0 text-muted text-decoration-none lh-1">
            <img src="images/logo_hri.png" width="50" height="20">
        </a>
        <span class="mb-3 mb-md-0 text-body-secondary">© 2024 Honda Research Institute Japan</span>
    </div>
</footer>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.min.js"></script>
<!-- <script src="/2024/js/bootstrap.min.js"></script> -->
</body>
</html>